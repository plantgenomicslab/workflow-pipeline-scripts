# Introduction
[Snakemake](https://snakemake.readthedocs.io/en/stable/) was used in an attempt to trim, align, perform differential expression analysis, and ultimately quality check the final results. The experience was a bit rough.

Initially, the [rna-seq-star-deseq2](https://github.com/snakemake-workflows/rna-seq-star-deseq2) and [dna-seq-gatk-variant-calling](https://github.com/snakemake-workflows/dna-seq-gatk-variant-calling) workflow pipelines were consulted. The objective was to take several Mesembryanthemum crystalinum sample sets, collected at different times and under control and stress conditions, and perform the differential expression analysis. Ultimately this was successful, however there were some workarounds necessary to effectively utilize snakemake. The biggest noted limitation was that the workflow had steps where the input and output were the same file, or were shared between steps. For example, an output BAM file needed to be indexed before a search could be performed. There are several different ways that this could be handled in snakemake, but they're not terrifically intuitive.

This repository contains the framework config and scripts at the end of the last run. Everything begins from the Snakefile. The HPC cluster configuration was used for parts of the run, consult the snakemake documentation for full usage details. The script wrapper.sh mostly contains the command-line invocation of snakemake.

NOTE: On SRA data pull, this is used to populate samples.tsv and units.tsv. In the config.yaml the SRA pull information is within the 'sradata' property. The 'method' may be 'fasterq' which uses fasterq-dump to pull and partition the data, or 'fastq' which uses a modified [parallel-fastq-dump](https://github.com/rvalieris/parallel-fastq-dump) script to pull the SRA data in chunks for partitioning. The 'outdir' specifies where the data is going to get written to. The 'sra_list' is a list of SRA records to pull. Each is described by it's SRA 'id', a 'label', a 'condition', and a 'unit'. The 'label' and 'condition' are written as 'sample' and 'condition' to the samples.tsv file. The 'label', and 'unit', along with the downloaded and partitioned SRA data file locations, are written to the units.tsv file. These data then are used to drive the rest of the snakemake pipeline. The SRA data will be downloaded and partitioned just once. Presence of the resultant partitioned data files inhibits subsequent downloads. Downloads are triggered with the invocation of snakemake, even when specifying '--dry-run', as the script is imported and run at the start of the Snakefile. Alternatively, the prepopulate_sra_data.py script may be invoked standalone to download and partition the data separately.
